# -*- coding: utf-8 -*-
"""
åŸºäº enhanced_production_features.csv è®­ç»ƒ LSTM å’Œ XGBoost å¤šç›®æ ‡å›å½’æ¨¡å‹
é¢„æµ‹ç›®æ ‡ï¼š
  - production_gap
  - filling_packaging_balance
  - bottleneck_severity
  - buffer_risk_score
"""

import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import StandardScaler
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
import xgboost as xgb
import joblib

# TensorFlow / Keras
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
tf.random.set_seed(42)

# ==============================
# 1. åŠ è½½ç‰¹å¾å·¥ç¨‹ç»“æœ
# ==============================
FEATURE_FILE = 'enhanced_production_features.csv'
if not os.path.exists(FEATURE_FILE):
    raise FileNotFoundError(f"è¯·å…ˆè¿è¡Œç‰¹å¾å·¥ç¨‹ç”Ÿæˆ {FEATURE_FILE}")

df = pd.read_csv(FEATURE_FILE, parse_dates=['æ—¥æœŸ'])
df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
print(f"âœ… åŠ è½½æ•°æ®å®Œæˆï¼Œå…± {len(df)} è¡Œï¼Œ{df.shape[1]} åˆ—")

# ==============================
# 2. å®šä¹‰ç‰¹å¾ä¸ç›®æ ‡
# ==============================
TARGET_COLS = [
    'production_gap',
    'filling_packaging_balance',
    'bottleneck_severity',
    'buffer_risk_score'
]

# æ’é™¤æ—¥æœŸå’Œæ ‡ç­¾åˆ—ï¼ˆåŒ…æ‹¬åˆ†ç±»æ ‡ç­¾å’Œç›®æ ‡åˆ—ï¼‰
EXCLUDE_COLS = ['æ—¥æœŸ'] + [
    'next_downtime', 'next_near_empty', 'is_bottleneck_fill', 'is_bottleneck_ster'
] + TARGET_COLS

FEATURE_COLS = [col for col in df.columns if col not in EXCLUDE_COLS]
print(f"ğŸ¯ ç‰¹å¾æ•°é‡: {len(FEATURE_COLS)}")
print(f"ğŸ¯ å›å½’ç›®æ ‡: {TARGET_COLS}")

X = df[FEATURE_COLS].values.astype(np.float32)
y = df[TARGET_COLS].values.astype(np.float32)

# ==============================
# 3. æ•°æ®åˆ’åˆ†ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
# ==============================
# ä¿ç•™æœ€å 30 å¤©ä½œä¸ºæµ‹è¯•é›†ï¼ˆæ—¶é—´åºåˆ—ä¸èƒ½éšæœºæ‰“ä¹±ï¼‰
TEST_DAYS = 30
split_idx = len(df) - TEST_DAYS

X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

print(f"ğŸ“Š è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

# ==============================
# 4. XGBoost å¤šè¾“å‡ºå›å½’
# ==============================
print("\nğŸš€ è®­ç»ƒ XGBoost å¤šè¾“å‡ºå›å½’æ¨¡å‹...")

xgb_model = MultiOutputRegressor(
    xgb.XGBRegressor(
        n_estimators=500,
        max_depth=6,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        objective='reg:squarederror'
    ),
    n_jobs=-1
)

xgb_model.fit(X_train, y_train)

# è¯„ä¼°
y_pred_xgb = xgb_model.predict(X_test)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print(f"âœ… XGBoost - MAE: {mae_xgb:.4f}, RÂ²: {r2_xgb:.4f}")

# ä¿å­˜
joblib.dump(xgb_model, 'xgb_multioutput_model.pkl')
print("ğŸ’¾ XGBoost æ¨¡å‹å·²ä¿å­˜ä¸º xgb_multioutput_model.pkl")

# ==============================
# 5. LSTM å¤šè¾“å‡ºå›å½’
# ==============================
print("\nğŸ§  è®­ç»ƒ LSTM å¤šè¾“å‡ºå›å½’æ¨¡å‹...")

# æ ‡å‡†åŒ–ï¼ˆLSTM å¯¹å°ºåº¦æ•æ„Ÿï¼‰
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train)
y_test_scaled = scaler_y.transform(y_test)

# æ„å»ºæ—¶é—´åºåˆ—æ»‘çª—ï¼šç”¨è¿‡å» SEQ_LEN å¤©é¢„æµ‹ç¬¬ SEQ_LEN+1 å¤©
SEQ_LEN = 7  # ä½¿ç”¨è¿‡å»7å¤©é¢„æµ‹æœªæ¥1å¤©

def create_sequences(X, y, seq_len):
    xs, ys = [], []
    for i in range(seq_len, len(X)):
        xs.append(X[i - seq_len:i])
        ys.append(y[i])
    return np.array(xs), np.array(ys)

X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, SEQ_LEN)
X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, SEQ_LEN)

print(f"ğŸ§© LSTM è¾“å…¥å½¢çŠ¶: {X_train_seq.shape} â†’ {y_train_seq.shape}")

# æ„å»ºæ¨¡å‹
input_layer = Input(shape=(SEQ_LEN, X_train_seq.shape[2]))
x = LSTM(64, return_sequences=True)(input_layer)
x = Dropout(0.3)(x)
x = LSTM(32)(x)
x = Dropout(0.2)(x)
output_layer = Dense(len(TARGET_COLS), activation='linear')(x)

lstm_model = Model(inputs=input_layer, outputs=output_layer)
lstm_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)

# å›è°ƒ
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# è®­ç»ƒ
history = lstm_model.fit(
    X_train_seq, y_train_seq,
    validation_data=(X_test_seq, y_test_seq),
    epochs=100,
    batch_size=16,
    callbacks=[early_stop],
    verbose=1
)

# é¢„æµ‹ï¼ˆåæ ‡å‡†åŒ–ï¼‰
y_pred_scaled = lstm_model.predict(X_test_seq)
y_pred_lstm = scaler_y.inverse_transform(y_pred_scaled)
y_test_actual = scaler_y.inverse_transform(y_test_seq)

mae_lstm = mean_absolute_error(y_test_actual, y_pred_lstm)
r2_lstm = r2_score(y_test_actual, y_pred_lstm)

print(f"âœ… LSTM - MAE: {mae_lstm:.4f}, RÂ²: {r2_lstm:.4f}")

# ä¿å­˜
lstm_model.save('lstm_multioutput_model.keras')
joblib.dump(scaler_X, 'lstm_scaler_X.pkl')
joblib.dump(scaler_y, 'lstm_scaler_y.pkl')
print("ğŸ’¾ LSTM æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨å·²ä¿å­˜")

# ==============================
# 6. ç»“æœå¯¹æ¯”
# ==============================
print("\n" + "="*50)
print("ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼ˆæµ‹è¯•é›†ï¼‰")
print("="*50)
print(f"{'æ¨¡å‹':<12} | {'MAE':<10} | {'RÂ²':<10}")
print("-"*50)
print(f"{'XGBoost':<12} | {mae_xgb:<10.4f} | {r2_xgb:<10.4f}")
print(f"{'LSTM':<12} | {mae_lstm:<10.4f} | {r2_lstm:<10.4f}")
print("="*50)

# å¯é€‰ï¼šä¿å­˜é¢„æµ‹ç»“æœç¤ºä¾‹
result_df = pd.DataFrame({
    'æ—¥æœŸ': df['æ—¥æœŸ'].iloc[-len(y_test_actual):].values,
    'çœŸå®_production_gap': y_test_actual[:, 0],
    'LSTM_pred_gap': y_pred_lstm[:, 0],
    'XGB_pred_gap': y_pred_xgb[-len(y_test_actual):, 0]
})
result_df.to_csv('prediction_comparison.csv', index=False, encoding='utf-8-sig')
print("\nğŸ“Š é¢„æµ‹å¯¹æ¯”å·²ä¿å­˜è‡³ prediction_comparison.csv")